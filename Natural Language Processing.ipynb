{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import nltk.corpus\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"For years Facebook gave some of the worlds largest technology companies more intrusive access to users personal data than it has disclosed, effectively exempting those business partners from its usual privacy rules, according to internal records and interviews The special arrangements are detailed in hundreds of pages of Facebook documents obtained by The New York Times. The records, generated in 2017 by the company internal system for tracking partnerships, provide the most complete picture yet of the social network data-sharing practices. They also underscore how personal data has become the most prized commodity of the digital age, traded on a vast scale by some of the most powerful companies in Silicon Valley and beyond. The exchange was intended to benefit everyone\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['For',\n",
       " 'years',\n",
       " 'Facebook',\n",
       " 'gave',\n",
       " 'some',\n",
       " 'of',\n",
       " 'the',\n",
       " 'worlds',\n",
       " 'largest',\n",
       " 'technology',\n",
       " 'companies',\n",
       " 'more',\n",
       " 'intrusive',\n",
       " 'access',\n",
       " 'to',\n",
       " 'users',\n",
       " 'personal',\n",
       " 'data',\n",
       " 'than',\n",
       " 'it',\n",
       " 'has',\n",
       " 'disclosed',\n",
       " ',',\n",
       " 'effectively',\n",
       " 'exempting',\n",
       " 'those',\n",
       " 'business',\n",
       " 'partners',\n",
       " 'from',\n",
       " 'its',\n",
       " 'usual',\n",
       " 'privacy',\n",
       " 'rules',\n",
       " ',',\n",
       " 'according',\n",
       " 'to',\n",
       " 'internal',\n",
       " 'records',\n",
       " 'and',\n",
       " 'interviews',\n",
       " 'The',\n",
       " 'special',\n",
       " 'arrangements',\n",
       " 'are',\n",
       " 'detailed',\n",
       " 'in',\n",
       " 'hundreds',\n",
       " 'of',\n",
       " 'pages',\n",
       " 'of',\n",
       " 'Facebook',\n",
       " 'documents',\n",
       " 'obtained',\n",
       " 'by',\n",
       " 'The',\n",
       " 'New',\n",
       " 'York',\n",
       " 'Times',\n",
       " '.',\n",
       " 'The',\n",
       " 'records',\n",
       " ',',\n",
       " 'generated',\n",
       " 'in',\n",
       " '2017',\n",
       " 'by',\n",
       " 'the',\n",
       " 'company',\n",
       " 'internal',\n",
       " 'system',\n",
       " 'for',\n",
       " 'tracking',\n",
       " 'partnerships',\n",
       " ',',\n",
       " 'provide',\n",
       " 'the',\n",
       " 'most',\n",
       " 'complete',\n",
       " 'picture',\n",
       " 'yet',\n",
       " 'of',\n",
       " 'the',\n",
       " 'social',\n",
       " 'network',\n",
       " 'data-sharing',\n",
       " 'practices',\n",
       " '.',\n",
       " 'They',\n",
       " 'also',\n",
       " 'underscore',\n",
       " 'how',\n",
       " 'personal',\n",
       " 'data',\n",
       " 'has',\n",
       " 'become',\n",
       " 'the',\n",
       " 'most',\n",
       " 'prized',\n",
       " 'commodity',\n",
       " 'of',\n",
       " 'the',\n",
       " 'digital',\n",
       " 'age',\n",
       " ',',\n",
       " 'traded',\n",
       " 'on',\n",
       " 'a',\n",
       " 'vast',\n",
       " 'scale',\n",
       " 'by',\n",
       " 'some',\n",
       " 'of',\n",
       " 'the',\n",
       " 'most',\n",
       " 'powerful',\n",
       " 'companies',\n",
       " 'in',\n",
       " 'Silicon',\n",
       " 'Valley',\n",
       " 'and',\n",
       " 'beyond',\n",
       " '.',\n",
       " 'The',\n",
       " 'exchange',\n",
       " 'was',\n",
       " 'intended',\n",
       " 'to',\n",
       " 'benefit',\n",
       " 'everyone']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = word_tokenize(text)\n",
    "tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, 129)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tokens), len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "fdist = FreqDist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'the': 14, 'of': 12, ',': 10, 'The': 8, 'to': 6, 'in': 6, 'by': 6, '.': 6, 'most': 6, 'Facebook': 4, ...})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in tokens:\n",
    "    fdist[i]=fdist[i]+1\n",
    "fdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 14),\n",
       " ('of', 12),\n",
       " (',', 10),\n",
       " ('The', 8),\n",
       " ('to', 6),\n",
       " ('in', 6),\n",
       " ('by', 6),\n",
       " ('.', 6),\n",
       " ('most', 6),\n",
       " ('Facebook', 4)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top10=fdist.most_common(10)\n",
    "top10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('For', 'years'),\n",
       " ('years', 'Facebook'),\n",
       " ('Facebook', 'gave'),\n",
       " ('gave', 'some'),\n",
       " ('some', 'of'),\n",
       " ('of', 'the'),\n",
       " ('the', 'worlds'),\n",
       " ('worlds', 'largest'),\n",
       " ('largest', 'technology'),\n",
       " ('technology', 'companies'),\n",
       " ('companies', 'more'),\n",
       " ('more', 'intrusive'),\n",
       " ('intrusive', 'access'),\n",
       " ('access', 'to'),\n",
       " ('to', 'users'),\n",
       " ('users', 'personal'),\n",
       " ('personal', 'data'),\n",
       " ('data', 'than'),\n",
       " ('than', 'it'),\n",
       " ('it', 'has'),\n",
       " ('has', 'disclosed'),\n",
       " ('disclosed', ','),\n",
       " (',', 'effectively'),\n",
       " ('effectively', 'exempting'),\n",
       " ('exempting', 'those'),\n",
       " ('those', 'business'),\n",
       " ('business', 'partners'),\n",
       " ('partners', 'from'),\n",
       " ('from', 'its'),\n",
       " ('its', 'usual'),\n",
       " ('usual', 'privacy'),\n",
       " ('privacy', 'rules'),\n",
       " ('rules', ','),\n",
       " (',', 'according'),\n",
       " ('according', 'to'),\n",
       " ('to', 'internal'),\n",
       " ('internal', 'records'),\n",
       " ('records', 'and'),\n",
       " ('and', 'interviews'),\n",
       " ('interviews', 'The'),\n",
       " ('The', 'special'),\n",
       " ('special', 'arrangements'),\n",
       " ('arrangements', 'are'),\n",
       " ('are', 'detailed'),\n",
       " ('detailed', 'in'),\n",
       " ('in', 'hundreds'),\n",
       " ('hundreds', 'of'),\n",
       " ('of', 'pages'),\n",
       " ('pages', 'of'),\n",
       " ('of', 'Facebook'),\n",
       " ('Facebook', 'documents'),\n",
       " ('documents', 'obtained'),\n",
       " ('obtained', 'by'),\n",
       " ('by', 'The'),\n",
       " ('The', 'New'),\n",
       " ('New', 'York'),\n",
       " ('York', 'Times'),\n",
       " ('Times', '.'),\n",
       " ('.', 'The'),\n",
       " ('The', 'records'),\n",
       " ('records', ','),\n",
       " (',', 'generated'),\n",
       " ('generated', 'in'),\n",
       " ('in', '2017'),\n",
       " ('2017', 'by'),\n",
       " ('by', 'the'),\n",
       " ('the', 'company'),\n",
       " ('company', 'internal'),\n",
       " ('internal', 'system'),\n",
       " ('system', 'for'),\n",
       " ('for', 'tracking'),\n",
       " ('tracking', 'partnerships'),\n",
       " ('partnerships', ','),\n",
       " (',', 'provide'),\n",
       " ('provide', 'the'),\n",
       " ('the', 'most'),\n",
       " ('most', 'complete'),\n",
       " ('complete', 'picture'),\n",
       " ('picture', 'yet'),\n",
       " ('yet', 'of'),\n",
       " ('of', 'the'),\n",
       " ('the', 'social'),\n",
       " ('social', 'network'),\n",
       " ('network', 'data-sharing'),\n",
       " ('data-sharing', 'practices'),\n",
       " ('practices', '.'),\n",
       " ('.', 'They'),\n",
       " ('They', 'also'),\n",
       " ('also', 'underscore'),\n",
       " ('underscore', 'how'),\n",
       " ('how', 'personal'),\n",
       " ('personal', 'data'),\n",
       " ('data', 'has'),\n",
       " ('has', 'become'),\n",
       " ('become', 'the'),\n",
       " ('the', 'most'),\n",
       " ('most', 'prized'),\n",
       " ('prized', 'commodity'),\n",
       " ('commodity', 'of'),\n",
       " ('of', 'the'),\n",
       " ('the', 'digital'),\n",
       " ('digital', 'age'),\n",
       " ('age', ','),\n",
       " (',', 'traded'),\n",
       " ('traded', 'on'),\n",
       " ('on', 'a'),\n",
       " ('a', 'vast'),\n",
       " ('vast', 'scale'),\n",
       " ('scale', 'by'),\n",
       " ('by', 'some'),\n",
       " ('some', 'of'),\n",
       " ('of', 'the'),\n",
       " ('the', 'most'),\n",
       " ('most', 'powerful'),\n",
       " ('powerful', 'companies'),\n",
       " ('companies', 'in'),\n",
       " ('in', 'Silicon'),\n",
       " ('Silicon', 'Valley'),\n",
       " ('Valley', 'and'),\n",
       " ('and', 'beyond'),\n",
       " ('beyond', '.'),\n",
       " ('.', 'The'),\n",
       " ('The', 'exchange'),\n",
       " ('exchange', 'was'),\n",
       " ('was', 'intended'),\n",
       " ('intended', 'to'),\n",
       " ('to', 'benefit'),\n",
       " ('benefit', 'everyone')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(nltk.bigrams(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('For', 'years', 'Facebook'),\n",
       " ('years', 'Facebook', 'gave'),\n",
       " ('Facebook', 'gave', 'some'),\n",
       " ('gave', 'some', 'of'),\n",
       " ('some', 'of', 'the'),\n",
       " ('of', 'the', 'worlds'),\n",
       " ('the', 'worlds', 'largest'),\n",
       " ('worlds', 'largest', 'technology'),\n",
       " ('largest', 'technology', 'companies'),\n",
       " ('technology', 'companies', 'more'),\n",
       " ('companies', 'more', 'intrusive'),\n",
       " ('more', 'intrusive', 'access'),\n",
       " ('intrusive', 'access', 'to'),\n",
       " ('access', 'to', 'users'),\n",
       " ('to', 'users', 'personal'),\n",
       " ('users', 'personal', 'data'),\n",
       " ('personal', 'data', 'than'),\n",
       " ('data', 'than', 'it'),\n",
       " ('than', 'it', 'has'),\n",
       " ('it', 'has', 'disclosed'),\n",
       " ('has', 'disclosed', ','),\n",
       " ('disclosed', ',', 'effectively'),\n",
       " (',', 'effectively', 'exempting'),\n",
       " ('effectively', 'exempting', 'those'),\n",
       " ('exempting', 'those', 'business'),\n",
       " ('those', 'business', 'partners'),\n",
       " ('business', 'partners', 'from'),\n",
       " ('partners', 'from', 'its'),\n",
       " ('from', 'its', 'usual'),\n",
       " ('its', 'usual', 'privacy'),\n",
       " ('usual', 'privacy', 'rules'),\n",
       " ('privacy', 'rules', ','),\n",
       " ('rules', ',', 'according'),\n",
       " (',', 'according', 'to'),\n",
       " ('according', 'to', 'internal'),\n",
       " ('to', 'internal', 'records'),\n",
       " ('internal', 'records', 'and'),\n",
       " ('records', 'and', 'interviews'),\n",
       " ('and', 'interviews', 'The'),\n",
       " ('interviews', 'The', 'special'),\n",
       " ('The', 'special', 'arrangements'),\n",
       " ('special', 'arrangements', 'are'),\n",
       " ('arrangements', 'are', 'detailed'),\n",
       " ('are', 'detailed', 'in'),\n",
       " ('detailed', 'in', 'hundreds'),\n",
       " ('in', 'hundreds', 'of'),\n",
       " ('hundreds', 'of', 'pages'),\n",
       " ('of', 'pages', 'of'),\n",
       " ('pages', 'of', 'Facebook'),\n",
       " ('of', 'Facebook', 'documents'),\n",
       " ('Facebook', 'documents', 'obtained'),\n",
       " ('documents', 'obtained', 'by'),\n",
       " ('obtained', 'by', 'The'),\n",
       " ('by', 'The', 'New'),\n",
       " ('The', 'New', 'York'),\n",
       " ('New', 'York', 'Times'),\n",
       " ('York', 'Times', '.'),\n",
       " ('Times', '.', 'The'),\n",
       " ('.', 'The', 'records'),\n",
       " ('The', 'records', ','),\n",
       " ('records', ',', 'generated'),\n",
       " (',', 'generated', 'in'),\n",
       " ('generated', 'in', '2017'),\n",
       " ('in', '2017', 'by'),\n",
       " ('2017', 'by', 'the'),\n",
       " ('by', 'the', 'company'),\n",
       " ('the', 'company', 'internal'),\n",
       " ('company', 'internal', 'system'),\n",
       " ('internal', 'system', 'for'),\n",
       " ('system', 'for', 'tracking'),\n",
       " ('for', 'tracking', 'partnerships'),\n",
       " ('tracking', 'partnerships', ','),\n",
       " ('partnerships', ',', 'provide'),\n",
       " (',', 'provide', 'the'),\n",
       " ('provide', 'the', 'most'),\n",
       " ('the', 'most', 'complete'),\n",
       " ('most', 'complete', 'picture'),\n",
       " ('complete', 'picture', 'yet'),\n",
       " ('picture', 'yet', 'of'),\n",
       " ('yet', 'of', 'the'),\n",
       " ('of', 'the', 'social'),\n",
       " ('the', 'social', 'network'),\n",
       " ('social', 'network', 'data-sharing'),\n",
       " ('network', 'data-sharing', 'practices'),\n",
       " ('data-sharing', 'practices', '.'),\n",
       " ('practices', '.', 'They'),\n",
       " ('.', 'They', 'also'),\n",
       " ('They', 'also', 'underscore'),\n",
       " ('also', 'underscore', 'how'),\n",
       " ('underscore', 'how', 'personal'),\n",
       " ('how', 'personal', 'data'),\n",
       " ('personal', 'data', 'has'),\n",
       " ('data', 'has', 'become'),\n",
       " ('has', 'become', 'the'),\n",
       " ('become', 'the', 'most'),\n",
       " ('the', 'most', 'prized'),\n",
       " ('most', 'prized', 'commodity'),\n",
       " ('prized', 'commodity', 'of'),\n",
       " ('commodity', 'of', 'the'),\n",
       " ('of', 'the', 'digital'),\n",
       " ('the', 'digital', 'age'),\n",
       " ('digital', 'age', ','),\n",
       " ('age', ',', 'traded'),\n",
       " (',', 'traded', 'on'),\n",
       " ('traded', 'on', 'a'),\n",
       " ('on', 'a', 'vast'),\n",
       " ('a', 'vast', 'scale'),\n",
       " ('vast', 'scale', 'by'),\n",
       " ('scale', 'by', 'some'),\n",
       " ('by', 'some', 'of'),\n",
       " ('some', 'of', 'the'),\n",
       " ('of', 'the', 'most'),\n",
       " ('the', 'most', 'powerful'),\n",
       " ('most', 'powerful', 'companies'),\n",
       " ('powerful', 'companies', 'in'),\n",
       " ('companies', 'in', 'Silicon'),\n",
       " ('in', 'Silicon', 'Valley'),\n",
       " ('Silicon', 'Valley', 'and'),\n",
       " ('Valley', 'and', 'beyond'),\n",
       " ('and', 'beyond', '.'),\n",
       " ('beyond', '.', 'The'),\n",
       " ('.', 'The', 'exchange'),\n",
       " ('The', 'exchange', 'was'),\n",
       " ('exchange', 'was', 'intended'),\n",
       " ('was', 'intended', 'to'),\n",
       " ('intended', 'to', 'benefit'),\n",
       " ('to', 'benefit', 'everyone')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(nltk.trigrams(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('For', 'years', 'Facebook', 'gave'),\n",
       " ('years', 'Facebook', 'gave', 'some'),\n",
       " ('Facebook', 'gave', 'some', 'of'),\n",
       " ('gave', 'some', 'of', 'the'),\n",
       " ('some', 'of', 'the', 'worlds'),\n",
       " ('of', 'the', 'worlds', 'largest'),\n",
       " ('the', 'worlds', 'largest', 'technology'),\n",
       " ('worlds', 'largest', 'technology', 'companies'),\n",
       " ('largest', 'technology', 'companies', 'more'),\n",
       " ('technology', 'companies', 'more', 'intrusive'),\n",
       " ('companies', 'more', 'intrusive', 'access'),\n",
       " ('more', 'intrusive', 'access', 'to'),\n",
       " ('intrusive', 'access', 'to', 'users'),\n",
       " ('access', 'to', 'users', 'personal'),\n",
       " ('to', 'users', 'personal', 'data'),\n",
       " ('users', 'personal', 'data', 'than'),\n",
       " ('personal', 'data', 'than', 'it'),\n",
       " ('data', 'than', 'it', 'has'),\n",
       " ('than', 'it', 'has', 'disclosed'),\n",
       " ('it', 'has', 'disclosed', ','),\n",
       " ('has', 'disclosed', ',', 'effectively'),\n",
       " ('disclosed', ',', 'effectively', 'exempting'),\n",
       " (',', 'effectively', 'exempting', 'those'),\n",
       " ('effectively', 'exempting', 'those', 'business'),\n",
       " ('exempting', 'those', 'business', 'partners'),\n",
       " ('those', 'business', 'partners', 'from'),\n",
       " ('business', 'partners', 'from', 'its'),\n",
       " ('partners', 'from', 'its', 'usual'),\n",
       " ('from', 'its', 'usual', 'privacy'),\n",
       " ('its', 'usual', 'privacy', 'rules'),\n",
       " ('usual', 'privacy', 'rules', ','),\n",
       " ('privacy', 'rules', ',', 'according'),\n",
       " ('rules', ',', 'according', 'to'),\n",
       " (',', 'according', 'to', 'internal'),\n",
       " ('according', 'to', 'internal', 'records'),\n",
       " ('to', 'internal', 'records', 'and'),\n",
       " ('internal', 'records', 'and', 'interviews'),\n",
       " ('records', 'and', 'interviews', 'The'),\n",
       " ('and', 'interviews', 'The', 'special'),\n",
       " ('interviews', 'The', 'special', 'arrangements'),\n",
       " ('The', 'special', 'arrangements', 'are'),\n",
       " ('special', 'arrangements', 'are', 'detailed'),\n",
       " ('arrangements', 'are', 'detailed', 'in'),\n",
       " ('are', 'detailed', 'in', 'hundreds'),\n",
       " ('detailed', 'in', 'hundreds', 'of'),\n",
       " ('in', 'hundreds', 'of', 'pages'),\n",
       " ('hundreds', 'of', 'pages', 'of'),\n",
       " ('of', 'pages', 'of', 'Facebook'),\n",
       " ('pages', 'of', 'Facebook', 'documents'),\n",
       " ('of', 'Facebook', 'documents', 'obtained'),\n",
       " ('Facebook', 'documents', 'obtained', 'by'),\n",
       " ('documents', 'obtained', 'by', 'The'),\n",
       " ('obtained', 'by', 'The', 'New'),\n",
       " ('by', 'The', 'New', 'York'),\n",
       " ('The', 'New', 'York', 'Times'),\n",
       " ('New', 'York', 'Times', '.'),\n",
       " ('York', 'Times', '.', 'The'),\n",
       " ('Times', '.', 'The', 'records'),\n",
       " ('.', 'The', 'records', ','),\n",
       " ('The', 'records', ',', 'generated'),\n",
       " ('records', ',', 'generated', 'in'),\n",
       " (',', 'generated', 'in', '2017'),\n",
       " ('generated', 'in', '2017', 'by'),\n",
       " ('in', '2017', 'by', 'the'),\n",
       " ('2017', 'by', 'the', 'company'),\n",
       " ('by', 'the', 'company', 'internal'),\n",
       " ('the', 'company', 'internal', 'system'),\n",
       " ('company', 'internal', 'system', 'for'),\n",
       " ('internal', 'system', 'for', 'tracking'),\n",
       " ('system', 'for', 'tracking', 'partnerships'),\n",
       " ('for', 'tracking', 'partnerships', ','),\n",
       " ('tracking', 'partnerships', ',', 'provide'),\n",
       " ('partnerships', ',', 'provide', 'the'),\n",
       " (',', 'provide', 'the', 'most'),\n",
       " ('provide', 'the', 'most', 'complete'),\n",
       " ('the', 'most', 'complete', 'picture'),\n",
       " ('most', 'complete', 'picture', 'yet'),\n",
       " ('complete', 'picture', 'yet', 'of'),\n",
       " ('picture', 'yet', 'of', 'the'),\n",
       " ('yet', 'of', 'the', 'social'),\n",
       " ('of', 'the', 'social', 'network'),\n",
       " ('the', 'social', 'network', 'data-sharing'),\n",
       " ('social', 'network', 'data-sharing', 'practices'),\n",
       " ('network', 'data-sharing', 'practices', '.'),\n",
       " ('data-sharing', 'practices', '.', 'They'),\n",
       " ('practices', '.', 'They', 'also'),\n",
       " ('.', 'They', 'also', 'underscore'),\n",
       " ('They', 'also', 'underscore', 'how'),\n",
       " ('also', 'underscore', 'how', 'personal'),\n",
       " ('underscore', 'how', 'personal', 'data'),\n",
       " ('how', 'personal', 'data', 'has'),\n",
       " ('personal', 'data', 'has', 'become'),\n",
       " ('data', 'has', 'become', 'the'),\n",
       " ('has', 'become', 'the', 'most'),\n",
       " ('become', 'the', 'most', 'prized'),\n",
       " ('the', 'most', 'prized', 'commodity'),\n",
       " ('most', 'prized', 'commodity', 'of'),\n",
       " ('prized', 'commodity', 'of', 'the'),\n",
       " ('commodity', 'of', 'the', 'digital'),\n",
       " ('of', 'the', 'digital', 'age'),\n",
       " ('the', 'digital', 'age', ','),\n",
       " ('digital', 'age', ',', 'traded'),\n",
       " ('age', ',', 'traded', 'on'),\n",
       " (',', 'traded', 'on', 'a'),\n",
       " ('traded', 'on', 'a', 'vast'),\n",
       " ('on', 'a', 'vast', 'scale'),\n",
       " ('a', 'vast', 'scale', 'by'),\n",
       " ('vast', 'scale', 'by', 'some'),\n",
       " ('scale', 'by', 'some', 'of'),\n",
       " ('by', 'some', 'of', 'the'),\n",
       " ('some', 'of', 'the', 'most'),\n",
       " ('of', 'the', 'most', 'powerful'),\n",
       " ('the', 'most', 'powerful', 'companies'),\n",
       " ('most', 'powerful', 'companies', 'in'),\n",
       " ('powerful', 'companies', 'in', 'Silicon'),\n",
       " ('companies', 'in', 'Silicon', 'Valley'),\n",
       " ('in', 'Silicon', 'Valley', 'and'),\n",
       " ('Silicon', 'Valley', 'and', 'beyond'),\n",
       " ('Valley', 'and', 'beyond', '.'),\n",
       " ('and', 'beyond', '.', 'The'),\n",
       " ('beyond', '.', 'The', 'exchange'),\n",
       " ('.', 'The', 'exchange', 'was'),\n",
       " ('The', 'exchange', 'was', 'intended'),\n",
       " ('exchange', 'was', 'intended', 'to'),\n",
       " ('was', 'intended', 'to', 'benefit'),\n",
       " ('intended', 'to', 'benefit', 'everyone')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(nltk.ngrams(tokens,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming is the process of reducing a word to it's word stem by cutting off the beginning or the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "pst=PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('win', 'studi', 'buy')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pst.stem(\"winning\"),pst.stem(\"studies\"),pst.stem(\"buying\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatization is the process of reducing words into their lemma or dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import wordnet, WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_stem = [\"cats\",\"cacti\",\"geese\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cats : cat\n",
      "cacti : cactus\n",
      "geese : goose\n"
     ]
    }
   ],
   "source": [
    "for i in words_to_stem:\n",
    "    print(i + \" : \" + lemmatizer.lemmatize(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part of speech tagging words are categorized into 8 parts of speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('What', 'WP')]\n",
      "[('are', 'VBP')]\n",
      "[('you', 'PRP')]\n",
      "[('trying', 'VBG')]\n",
      "[('to', 'TO')]\n",
      "[('say', 'VB')]\n",
      "[('!', '.')]\n",
      "[('We', 'PRP')]\n",
      "[('are', 'VBP')]\n",
      "[('all', 'DT')]\n",
      "[('here', 'RB')]\n",
      "[('to', 'TO')]\n",
      "[('learn', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "text = \"What are you trying to say! We are all here to learn\"\n",
    "for i in word_tokenize(text) :\n",
    "    print(nltk.pos_tag([i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Named Entity Recognition is the process of taking a string of text as input and identify relevant nons (people, places, and organizations) that are mentioned in that string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ne_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"John lives in New York\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('John', 'NNP'),\n",
       " ('lives', 'VBZ'),\n",
       " ('in', 'IN'),\n",
       " ('New', 'NNP'),\n",
       " ('York', 'NNP')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_tag = nltk.pos_tag(word_tokenize(text))\n",
    "text_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S (PERSON John/NNP) lives/VBZ in/IN (GPE New/NNP York/NNP))\n"
     ]
    }
   ],
   "source": [
    "text_ner = ne_chunk(text_tag)\n",
    "print(text_ner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"this is our corpus.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this\n",
      "is\n",
      "our\n",
      "corpus\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for token in doc :\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "our"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token = doc[2]\n",
    "token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "our corpus."
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "span = doc[2:5]\n",
    "span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 this\n",
      "1 is\n",
      "2 our\n",
      "3 corpus\n",
      "4 .\n"
     ]
    }
   ],
   "source": [
    "for token in doc :\n",
    "    print(token.i,token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "John likes to drink 2 coffees per a day, it costs 1$"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(\"John likes to drink 2 coffees per a day, it costs 1$\")\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 John PROPN\n",
      "1 likes VERB\n",
      "2 to PART\n",
      "3 drink VERB\n",
      "4 2 NUM\n",
      "5 coffees NOUN\n",
      "6 per ADP\n",
      "7 a DET\n",
      "8 day NOUN\n",
      "9 , PUNCT\n",
      "10 it PRON\n",
      "11 costs VERB\n",
      "12 1 NUM\n",
      "13 $ SYM\n"
     ]
    }
   ],
   "source": [
    "for token in doc : \n",
    "    print(token.i, token.text,token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "John PERSON\n",
      "2 CARDINAL\n",
      "1$ MONEY\n"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"John likes to share his knowledge\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = [{'LEMMA':'share'},{'ORTH':'his'}]\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add('white_Pattern',[pattern])\n",
    "matches = matcher(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "share his\n"
     ]
    }
   ],
   "source": [
    "for _,start,end in matches:\n",
    "    span = doc[start:end]\n",
    "    print(span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"2018 FIFA world cup : France won!!!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = [{'LEMMA':'share'},{'ORTH':'his'}]\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add('white_Pattern',[pattern])\n",
    "matches = matcher(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
