{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Natural Language Processing\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import nltk.corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Natural language processing is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural', 'language', 'processing', 'is', 'a', 'subfield', 'of', 'linguistics', ',', 'computer', 'science', ',', 'and', 'artificial', 'intelligence', 'concerned', 'with', 'the', 'interactions', 'between', 'computers', 'and', 'human', 'language', ',', 'in', 'particular', 'how', 'to', 'program', 'computers', 'to', 'process', 'and', 'analyze', 'large', 'amounts', 'of', 'natural', 'language', 'data', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, 42)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tokens), len(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'language': 3, ',': 3, 'and': 3, 'of': 2, 'computers': 2, 'to': 2, 'Natural': 1, 'processing': 1, 'is': 1, 'a': 1, ...})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.probability import FreqDist\n",
    "fdist = FreqDist()\n",
    "for i in tokens:\n",
    "    fdist[i]=fdist[i]+1\n",
    "fdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('language', 3), (',', 3), ('and', 3), ('of', 2), ('computers', 2)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top5=fdist.most_common(5)\n",
    "top5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural', 'language', 'processing', 'is', 'a', 'subfield', 'of', 'linguistics', ',', 'computer', 'science', ',', 'and', 'artificial', 'intelligence']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Natural', 'language'),\n",
       " ('language', 'processing'),\n",
       " ('processing', 'is'),\n",
       " ('is', 'a'),\n",
       " ('a', 'subfield'),\n",
       " ('subfield', 'of'),\n",
       " ('of', 'linguistics'),\n",
       " ('linguistics', ','),\n",
       " (',', 'computer'),\n",
       " ('computer', 'science'),\n",
       " ('science', ','),\n",
       " (',', 'and'),\n",
       " ('and', 'artificial'),\n",
       " ('artificial', 'intelligence')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tokens[:15])\n",
    "list(nltk.bigrams(tokens[:15]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Natural', 'language', 'processing'),\n",
       " ('language', 'processing', 'is'),\n",
       " ('processing', 'is', 'a'),\n",
       " ('is', 'a', 'subfield'),\n",
       " ('a', 'subfield', 'of'),\n",
       " ('subfield', 'of', 'linguistics'),\n",
       " ('of', 'linguistics', ','),\n",
       " ('linguistics', ',', 'computer'),\n",
       " (',', 'computer', 'science'),\n",
       " ('computer', 'science', ','),\n",
       " ('science', ',', 'and'),\n",
       " (',', 'and', 'artificial'),\n",
       " ('and', 'artificial', 'intelligence')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(nltk.trigrams(tokens[:15]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Natural', 'language', 'processing', 'is'),\n",
       " ('language', 'processing', 'is', 'a'),\n",
       " ('processing', 'is', 'a', 'subfield'),\n",
       " ('is', 'a', 'subfield', 'of'),\n",
       " ('a', 'subfield', 'of', 'linguistics'),\n",
       " ('subfield', 'of', 'linguistics', ','),\n",
       " ('of', 'linguistics', ',', 'computer'),\n",
       " ('linguistics', ',', 'computer', 'science'),\n",
       " (',', 'computer', 'science', ','),\n",
       " ('computer', 'science', ',', 'and'),\n",
       " ('science', ',', 'and', 'artificial'),\n",
       " (',', 'and', 'artificial', 'intelligence')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(nltk.ngrams(tokens[:15],4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming is the process of reducing a word to it's word stem by cutting off the beginning or the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "pst=PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('win', 'studi', 'buy')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pst.stem(\"winning\"),pst.stem(\"studies\"),pst.stem(\"buying\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatization is the process of grouping together the different inflected forms of a word so they can be analyzed as a single item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import wordnet, WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "words= [\"widget\",\"flowers\",\"geese\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "widget : widget\n",
      "flowers : flower\n",
      "geese : goose\n"
     ]
    }
   ],
   "source": [
    "for i in words:\n",
    "    print(i + \" : \" + lemmatizer.lemmatize(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part of Speech Tagging "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('What', 'WP')]\n",
      "[('are', 'VBP')]\n",
      "[('you', 'PRP')]\n",
      "[('trying', 'VBG')]\n",
      "[('to', 'TO')]\n",
      "[('say', 'VB')]\n",
      "[('!', '.')]\n",
      "[('We', 'PRP')]\n",
      "[('are', 'VBP')]\n",
      "[('all', 'DT')]\n",
      "[('here', 'RB')]\n",
      "[('to', 'TO')]\n",
      "[('learn', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "text = \"What are you trying to say! We are all here to learn\"\n",
    "for i in word_tokenize(text) :\n",
    "    print(nltk.pos_tag([i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Named Entity Recognition is the process of taking a string of text as input and identify relevant nouns that are mentioned in that string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ne_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S (PERSON John/NNP) lives/VBZ in/IN (GPE New/NNP York/NNP))\n"
     ]
    }
   ],
   "source": [
    "text = \"John lives in New York\"\n",
    "text_ner = ne_chunk(nltk.pos_tag(word_tokenize(text)))\n",
    "print(text_ner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this\n",
      "is\n",
      "our\n",
      "corpus\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"this is our corpus.\")\n",
    "for token in doc :\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 this\n",
      "1 is\n",
      "2 our\n",
      "3 corpus\n",
      "4 .\n"
     ]
    }
   ],
   "source": [
    "for token in doc :\n",
    "    print(token.i,token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "An average cup of coffee costs 2.70$"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(\"An average cup of coffee costs 2.70$\")\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 An DET\n",
      "1 average ADJ\n",
      "2 cup NOUN\n",
      "3 of ADP\n",
      "4 coffee NOUN\n",
      "5 costs VERB\n",
      "6 2.70 NUM\n",
      "7 $ SYM\n"
     ]
    }
   ],
   "source": [
    "for token in doc : \n",
    "    print(token.i, token.text,token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.70$ MONEY\n"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"John likes to share his knowledge\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = [{'LEMMA':'share'},{'ORTH':'his'}]\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add('white_Pattern',[pattern])\n",
    "matches = matcher(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "share his\n"
     ]
    }
   ],
   "source": [
    "for _,start,end in matches:\n",
    "    span = doc[start:end]\n",
    "    print(span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"2018 FIFA world cup : France won!!!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = [{'IS_DIGIT':True},{'LOWER':'fifa'},{'LOWER':'world'},{'LOWER':'cup'}]\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add('white_Pattern',[pattern])\n",
    "matches = matcher(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018 FIFA world cup\n"
     ]
    }
   ],
   "source": [
    "for _,start,end in matches:\n",
    "    span = doc[start:end]\n",
    "    print(span.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
